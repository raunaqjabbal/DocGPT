{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "KJGku2yoHI6x"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain tiktoken duckduckgo-search openai\n",
        "%pip install -q langchain huggingface_hub tiktoken\n",
        "%pip install -q chromadb\n",
        "%pip install -q PyPDF2 pypdf sentence_transformers\n",
        "%pip install -qU FlagEmbedding\n",
        "%pip install -qU chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip db.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ksjh-9FTHQCk",
        "outputId": "97c49ddf-9a6d-4d45-ade1-9f78edd52d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HYL5jEOHHI60"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v4LVeNKaHI61"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
        "\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor, create_csv_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import OpenAI, LLMChain, PromptTemplate\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "from langchain.chains.summarize import load_summarize_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bqEhgLplHI62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Raunaq\\Desktop\\Mercor\\textbase\\.venv\\lib\\site-packages\\langchain\\llms\\openai.py:200: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "c:\\Users\\Raunaq\\Desktop\\Mercor\\textbase\\.venv\\lib\\site-packages\\langchain\\llms\\openai.py:787: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EfXGS3fNKJ4l"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Raunaq\\Desktop\\Mercor\\textbase\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_name= \"thenlper/gte-small\"\n",
        "\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "\n",
        "embedding = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    # model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('doc_embedding.pickle', 'wb') as pkl:\n",
        "    pickle.dump(embedding, pkl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B7ldsp3zKg4W"
      },
      "outputs": [],
      "source": [
        "# loader = DirectoryLoader('./drive/MyDrive/ICT/resources', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "# documents = loader.load()\n",
        "\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "# texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# persist_directory = 'drive/MyDrive/ICT/db'\n",
        "\n",
        "# vectordb = Chroma.from_documents(documents=texts,\n",
        "#                                  embedding=embedding,\n",
        "#                                  persist_directory=persist_directory)\n",
        "\n",
        "# vectordb.persist()\n",
        "\n",
        "persist_directory=\"db\"\n",
        "\n",
        "vectordb = Chroma(persist_directory=persist_directory,\n",
        "                  embedding_function=embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rmvxPKifKtIR"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever)\n",
        "                                  # return_source_documents=True)\n",
        "\n",
        "summarize_template = \"\"\"Being a medical professional, write a concise bullet point summary do not leave out any facts or opinions:\n",
        "{text}\n",
        "Summary: \"\"\"\n",
        "\n",
        "summarize_prompt = PromptTemplate(template=summarize_template,\n",
        "                        input_variables=[\"text\"])\n",
        "\n",
        "summarize_chain = load_summarize_chain(llm,\n",
        "                             chain_type=\"stuff\", # refine, map-reduce, stuff, use refine for documents\n",
        "                             prompt=summarize_prompt)\n",
        "csv_agent = create_csv_agent(OpenAI(temperature=0),\n",
        "                         'medical_records.csv',\n",
        "                         verbose=True,\n",
        "                             max_iterations=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A7fA9qKJM75M"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=120):\n",
        "  lines = text.split('\\n')\n",
        "  wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "  wrapped_text = '\\n'.join(wrapped_lines)\n",
        "  return wrapped_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bkKFhHJ1k7bp"
      },
      "outputs": [],
      "source": [
        "def duck_wrapper(input_text):\n",
        "  search_results = DuckDuckGoSearchRun()(f\"{input_text}\")\n",
        "  return search_results\n",
        "\n",
        "def search(input_text):\n",
        "  input_text2 = qa_chain.run(input_text)\n",
        "  input_text3 = duck_wrapper(input_text+\" \"+input_text2)\n",
        "  print(\"INPUT TEXT: \",input_text)\n",
        "  return \"Answer 1: \\n\"+input_text2+\" \\nAnswer 2: \\n\"+input_text3\n",
        "\n",
        "\n",
        "tools = [\n",
        "\n",
        "    Tool(\n",
        "        name = \"Search WebMD\",\n",
        "        func=duck_wrapper,\n",
        "        description=\"useful for when you need to find answers about other non medical questions\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search,\n",
        "        description=\"Useful when you need to find out answers to a medical questions\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"Document Summarizer\",\n",
        "        func=summarize_chain.run,\n",
        "        description=\"Useful when you need to summarize reports, documents, paragraphs\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"Database Reader\",\n",
        "        func=csv_agent.run,\n",
        "        description=\"Write python code to retrieve data from the hospital database\"\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XjrwoB53eHa3"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "        memory_key=\"chat_history\", llm=llm, max_token_limit=50, return_messages=True\n",
        ")\n",
        "conversational_agent = initialize_agent(\n",
        "    agent='conversational-react-description',\n",
        "    tools=tools,\n",
        "    llm=OpenAI(temperature=0),\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method='generate',\n",
        "    memory=memory,\n",
        ")\n",
        "new_template = '''Assistant is a friendly medical professional that answers correctly and tells when it does not know the answer and does not hallucinate. Assistant does not tell the user to consult other doctors. \n",
        "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\n",
        "Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
        "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
        "\n",
        "'''\n",
        "\n",
        "import regex as re\n",
        "a = re.search(r'TOOLS:', conversational_agent.agent.llm_chain.prompt.template)\n",
        "conversational_agent.agent.llm_chain.prompt.template=new_template+conversational_agent.agent.llm_chain.prompt.template[a.start():]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Assistant is a friendly medical professional that answers correctly and tells when it does not know the answer and does not hallucinate. Assistant does not tell the user to consult other doctors. \n",
            "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\n",
            "Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
            "\n",
            "TOOLS:\n",
            "------\n",
            "\n",
            "Assistant has access to the following tools:\n",
            "\n",
            "> Search WebMD: useful for when you need to find answers about other non medical questions\n",
            "> Search: Useful when you need to find out answers to a medical questions\n",
            "> Document Summarizer: Useful when you need to summarize reports, documents, paragraphs\n",
            "> Database Reader: Write python code to retrieve data from the hospital database\n",
            "\n",
            "To use a tool, please use the following format:\n",
            "\n",
            "```\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: the action to take, should be one of [Search WebMD, Search, Document Summarizer, Database Reader]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "```\n",
            "\n",
            "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
            "\n",
            "```\n",
            "Thought: Do I need to use a tool? No\n",
            "AI: [your response here]\n",
            "```\n",
            "\n",
            "Begin!\n",
            "\n",
            "Previous conversation history:\n",
            "{chat_history}\n",
            "\n",
            "New input: {input}\n",
            "{agent_scratchpad}\n"
          ]
        }
      ],
      "source": [
        "print(conversational_agent.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant is a large language model trained by OpenAI.\n",
            "\n",
            "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
            "\n",
            "TOOLS:\n",
            "------\n",
            "\n",
            "Assistant has access to the following tools:\n",
            "\n",
            "> Search WebMD: useful for when you need to find answers about other non medical questions\n",
            "> Search: Useful when you need to find out answers to a medical questions\n",
            "> Document Summarizer: Useful when you need to summarize reports, documents, paragraphs\n",
            "> Database Reader: Write python code to retrieve data from the hospital database\n",
            "\n",
            "To use a tool, please use the following format:\n",
            "\n",
            "```\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: the action to take, should be one of [Search WebMD, Search, Document Summarizer, Database Reader]\n",
            "Action Input: the input to the action\n",
            "Observation: the result of the action\n",
            "```\n",
            "\n",
            "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
            "\n",
            "```\n",
            "Thought: Do I need to use a tool? No\n",
            "AI: [your response here]\n",
            "```\n",
            "\n",
            "Begin!\n",
            "\n",
            "Previous conversation history:\n",
            "{chat_history}\n",
            "\n",
            "New input: {input}\n",
            "{agent_scratchpad}\n"
          ]
        }
      ],
      "source": [
        "print(conversational_agent.agent.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "9jD2qZdTrZR0",
        "outputId": "17248786-a675-4689-bc62-568c9652f9ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Thought: Do I need to use a tool? Yes\n",
            "Action: Database Reader\n",
            "Action Input: Count the number of unique patients\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to count the number of unique patient_id values\n",
            "Action: python_repl_ast\n",
            "Action Input: len(df['patient_id'].unique())\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m100000\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 100000\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3m100000\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
            "AI: There are 100000 unique patients in the database.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are 100000 unique patients in the database.'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_agent.run(\"how many unique patients are there\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "1JUaoIKmHI63",
        "outputId": "23e7ee8c-1e44-4a25-e583-1cd1c33b6215"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-73c7e104442d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZeroShotAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqa_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m agent_chain = AgentExecutor.from_agent_and_tools(\n\u001b[1;32m      8\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.validate_model\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/agents/agent.py\u001b[0m in \u001b[0;36mvalidate_prompt\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;34m\"\"\"Validate that prompt matches format.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"llm_chain\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"agent_scratchpad\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             logger.warning(\n",
            "\u001b[0;31mKeyError\u001b[0m: 'llm_chain'"
          ]
        }
      ],
      "source": [
        "agent_chain = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True, memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdiAofCOHI64",
        "outputId": "57f686b6-0b8f-47a0-f849-b2e43cfc5efd"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "MultiQueryRetriever.from_llm() missing 1 required positional argument: 'retriever'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Raunaq\\Desktop\\GPT.ipynb Cell 8\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raunaq/Desktop/GPT.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m logging\u001b[39m.\u001b[39mbasicConfig()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raunaq/Desktop/GPT.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mlangchain.retrievers.multi_query\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msetLevel(logging\u001b[39m.\u001b[39mINFO)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Raunaq/Desktop/GPT.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m retriever_from_llm \u001b[39m=\u001b[39m MultiQueryRetriever\u001b[39m.\u001b[39;49mfrom_llm(llm\u001b[39m=\u001b[39;49mOpenAI(temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m))\n",
            "\u001b[1;31mTypeError\u001b[0m: MultiQueryRetriever.from_llm() missing 1 required positional argument: 'retriever'"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(llm=OpenAI(temperature=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bXXJI_DHI65"
      },
      "outputs": [],
      "source": [
        "retriever_from_llm.run(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVBynjd9iCqb"
      },
      "outputs": [],
      "source": [
        "# # Set up the base template\n",
        "# template = \"\"\"AI is a compassionate medical professional talking to human and giving human advice.\n",
        "\n",
        "# You have access to the following tools:\n",
        "# {tools}\n",
        "\n",
        "# Use the following format:\n",
        "\n",
        "# Question: the input question you must answer\n",
        "# Thought: you should always think about what to do\n",
        "# Action: the action to take, should be one of [{tool_names}]\n",
        "# Action Input: the input to the action\n",
        "# Observation: the result of the action\n",
        "# ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "# Thought: I now know the final answer\n",
        "# Final Answer: the final answer to the original input question\n",
        "\n",
        "# You are AI, talking to Human in the below conversation. Remember to answer as a compansionate medical professional when giving your final answer.\n",
        "\n",
        "# {chat_history}\n",
        "\n",
        "\n",
        "# Human: {input}\n",
        "# {agent_scratchpad}\"\"\"\n",
        "\n",
        "# prefix = \"\"\"Have a conversation with a human as a medical professional, answering the following questions as best you can. You have access to the following tools:\"\"\"\n",
        "# suffix = \"\"\"Begin!\"\n",
        "\n",
        "# {chat_history}\n",
        "# Question: {input}\n",
        "# {agent_scratchpad}\"\"\"\n",
        "\n",
        "\n",
        "# prompt = ZeroShotAgent.create_prompt(\n",
        "#     tools,\n",
        "#     prefix=prefix,\n",
        "#     suffix=suffix,\n",
        "#     input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
